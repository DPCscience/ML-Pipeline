{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Workshop\n",
    "\n",
    "Here we will walk through an example of a machine learning workflow following five steps:\n",
    "\n",
    "1. Formulating a question\n",
    "2. Data collection/cleaning\n",
    "3. Feature engineering\n",
    "4. Model selection and hyperparameter tuning\n",
    "5. Model application\n",
    "\n",
    "For more information on the Shiu Lab's ML pipeline, check out the [README](https://github.com/ShiuLab/ML-Pipeline/blob/master/README.md).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up\n",
    "\n",
    "Check out this [guide](https://github.com/ShiuLab/ML-Pipeline/blob/master/Tutorial/README.md) to learn how to set up Jupyter notebook and the software needed to run the Shiu Lab's ML pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Formulating a question\n",
    "\n",
    "Topic: Specialized metabolism in tomato.\n",
    "\n",
    "Possible Questions:\n",
    "- How well can we predict if a gene is involved in specialized or general metabolism?\n",
    "- What features are most predictive of specialized metabolism?\n",
    "- Which unknown genes are likely involved in specialized metabolism?\n",
    "- Can information from a model system like Arabidopsis help us learn about specialized metabolism in tomato?\n",
    "\n",
    "\n",
    "What do we need:\n",
    "\n",
    "- Training data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data collection/cleaning\n",
    "\n",
    "Steps:\n",
    "   - A. Load in and look at the data\n",
    "   - B. Remove or impute n/a values and convert categorical features into a format suitable for ML modeling\n",
    "   - C. Define the testing set (these data will not be used at any stage during parameter selection or training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data (rows, cols):\n",
      "(2872, 565)\n",
      "\n",
      "Snapshot of data:\n",
      "                Class  Crubella_183_v1.0.csv  FamilySize FamilySize_cat  \\\n",
      "YP_008563134      gen                    0.0    0.010582         medium   \n",
      "XP_010327628      gen                    0.0    0.000000          small   \n",
      "XP_010327620  special                    0.0    0.052910         medium   \n",
      "XP_010327578      gen                    0.0         NaN            NaN   \n",
      "XP_010327494      gen                    1.0    0.021164         medium   \n",
      "\n",
      "              Transferase  SQS_PSY  Exo_endo_phos  IPT  Paxillaris_medKaKs  \\\n",
      "YP_008563134          0.0      NaN            0.0  0.0            0.179007   \n",
      "XP_010327628          NaN      NaN            0.0  0.0            0.179007   \n",
      "XP_010327620          0.0      NaN            0.0  0.0            0.237404   \n",
      "XP_010327578          0.0      NaN            0.0  0.0            0.237404   \n",
      "XP_010327494          0.0      0.0            0.0  0.0            0.092387   \n",
      "\n",
      "              GalKase_gal_bdg      ...        Polysacc_synt_2  \\\n",
      "YP_008563134                0      ...                      0   \n",
      "XP_010327628                0      ...                      0   \n",
      "XP_010327620                0      ...                      0   \n",
      "XP_010327578                0      ...                      0   \n",
      "XP_010327494                0      ...                      0   \n",
      "\n",
      "              Lysine_decarbox  Exostosin  Glyco_hydro_28  Lipase_3  Wax2_C  \\\n",
      "YP_008563134                0          0               0         0       0   \n",
      "XP_010327628                0          0               0         0       0   \n",
      "XP_010327620                0          0               0         0       0   \n",
      "XP_010327578                0          0               0         0       0   \n",
      "XP_010327494                0          0               0         0       0   \n",
      "\n",
      "              Pabies_medKaKs  Glutaredoxin  Osativa_medKaKs  Abhydrolase_6  \n",
      "YP_008563134             NaN             0         0.247425              0  \n",
      "XP_010327628        0.198686             0         0.247425              0  \n",
      "XP_010327620        0.161359             0              NaN              0  \n",
      "XP_010327578        0.161359             0         0.212226              0  \n",
      "XP_010327494        0.055363             0         0.179891              0  \n",
      "\n",
      "[5 rows x 565 columns]\n",
      "\n",
      " List of class labels\n",
      "['gen' 'special' 'unknown']\n"
     ]
    }
   ],
   "source": [
    "## A. Load in and look at the data\n",
    "\n",
    "d = pd.read_csv('data.txt', sep='\\t', index_col = 0)\n",
    "\n",
    "print('Shape of data (rows, cols):')\n",
    "print(d.shape)\n",
    "\n",
    "print('\\nSnapshot of data:')\n",
    "print(d.head())\n",
    "\n",
    "print('\\n List of class labels')\n",
    "print(d['Class'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes about this data:\n",
    "- Some columns have NAs! ML algorithms can NOT use features with NAs, so these will need to be imputed.\n",
    "- We have binary, continuous, and categorical features in this dataset. A perk of ML models is that they can integrate multiple datatypes in a single model. \n",
    "- However, before being used as input, a categorical feature needs to be converted into set binary features using an approach called [one-hot-encoding](https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding). \n",
    "\n",
    "### The ML_preprocessing.py script automates all this data cleaning. \n",
    "\n",
    "ML_preprocessing.py will:\n",
    "1. Drop columns if the number of NAs is above a certain threshold (default = 50%)\n",
    "2. Impute the remaining columns using the median, mean, or mode of the data present for that feature\n",
    "3. One-hot-encode  categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot of input data...\n",
      "                Class  Crubella_183_v1.0.csv  FamilySize FamilySize_cat  \\\n",
      "YP_008563134      gen                    0.0    0.010582         medium   \n",
      "XP_010327628      gen                    0.0    0.000000          small   \n",
      "XP_010327620  special                    0.0    0.052910         medium   \n",
      "XP_010327578      gen                    0.0         NaN            NaN   \n",
      "XP_010327494      gen                    1.0    0.021164         medium   \n",
      "\n",
      "              Transferase  SQS_PSY  Exo_endo_phos  IPT  Paxillaris_medKaKs  \\\n",
      "YP_008563134          0.0      NaN            0.0  0.0            0.179007   \n",
      "XP_010327628          NaN      NaN            0.0  0.0            0.179007   \n",
      "XP_010327620          0.0      NaN            0.0  0.0            0.237404   \n",
      "XP_010327578          0.0      NaN            0.0  0.0            0.237404   \n",
      "XP_010327494          0.0      0.0            0.0  0.0            0.092387   \n",
      "\n",
      "              GalKase_gal_bdg      ...        Polysacc_synt_2  \\\n",
      "YP_008563134                0      ...                      0   \n",
      "XP_010327628                0      ...                      0   \n",
      "XP_010327620                0      ...                      0   \n",
      "XP_010327578                0      ...                      0   \n",
      "XP_010327494                0      ...                      0   \n",
      "\n",
      "              Lysine_decarbox  Exostosin  Glyco_hydro_28  Lipase_3  Wax2_C  \\\n",
      "YP_008563134                0          0               0         0       0   \n",
      "XP_010327628                0          0               0         0       0   \n",
      "XP_010327620                0          0               0         0       0   \n",
      "XP_010327578                0          0               0         0       0   \n",
      "XP_010327494                0          0               0         0       0   \n",
      "\n",
      "              Pabies_medKaKs  Glutaredoxin  Osativa_medKaKs  Abhydrolase_6  \n",
      "YP_008563134             NaN             0         0.247425              0  \n",
      "XP_010327628        0.198686             0         0.247425              0  \n",
      "XP_010327620        0.161359             0              NaN              0  \n",
      "XP_010327578        0.161359             0         0.212226              0  \n",
      "XP_010327494        0.055363             0         0.179891              0  \n",
      "\n",
      "[5 rows x 565 columns]\n",
      "\n",
      "\n",
      "### Dropping/imputing NAs... ###\n",
      "\n",
      "Number of columns with NAs: 41\n",
      "Features dropped because missing > 0.50% of data: ['SQS_PSY']\n",
      "Number of columns to impute: 40\n",
      "\n",
      "\n",
      "### One Hot Encoding... ###\n",
      "\n",
      "\n",
      "Features to one-hot-encode: ['FamilySize_cat']\n",
      "Dataframe shape (rows, cols) before and after one-hot-encoding:\n",
      "Before: (2872, 563)\n",
      "After: (2872, 565)\n",
      "\n",
      "\n",
      "Snapshot of imputed data...\n",
      "                Class  Crubella_183_v1.0.csv  FamilySize  Transferase  \\\n",
      "YP_008563134      gen                    0.0    0.010582          0.0   \n",
      "XP_010327628      gen                    0.0    0.000000          0.0   \n",
      "XP_010327620  special                    0.0    0.052910          0.0   \n",
      "XP_010327578      gen                    0.0    0.015873          0.0   \n",
      "XP_010327494      gen                    1.0    0.021164          0.0   \n",
      "\n",
      "              Exo_endo_phos  IPT  Paxillaris_medKaKs  GalKase_gal_bdg  \\\n",
      "YP_008563134            0.0  0.0            0.179007                0   \n",
      "XP_010327628            0.0  0.0            0.179007                0   \n",
      "XP_010327620            0.0  0.0            0.237404                0   \n",
      "XP_010327578            0.0  0.0            0.237404                0   \n",
      "XP_010327494            0.0  0.0            0.092387                0   \n",
      "\n",
      "              Pkinase_Tyr  Smelongena_medKaKs          ...           \\\n",
      "YP_008563134          0.0            0.030057          ...            \n",
      "XP_010327628          1.0            0.030057          ...            \n",
      "XP_010327620          1.0            0.064836          ...            \n",
      "XP_010327578          0.0            0.064836          ...            \n",
      "XP_010327494          0.0            0.085421          ...            \n",
      "\n",
      "              Glyco_hydro_28  Lipase_3  Wax2_C  Pabies_medKaKs  Glutaredoxin  \\\n",
      "YP_008563134               0         0       0        0.161359             0   \n",
      "XP_010327628               0         0       0        0.198686             0   \n",
      "XP_010327620               0         0       0        0.161359             0   \n",
      "XP_010327578               0         0       0        0.161359             0   \n",
      "XP_010327494               0         0       0        0.055363             0   \n",
      "\n",
      "              Osativa_medKaKs  Abhydrolase_6  FamilySize_cat_large  \\\n",
      "YP_008563134         0.247425              0                     0   \n",
      "XP_010327628         0.247425              0                     0   \n",
      "XP_010327620         0.206460              0                     0   \n",
      "XP_010327578         0.212226              0                     0   \n",
      "XP_010327494         0.179891              0                     0   \n",
      "\n",
      "              FamilySize_cat_medium  FamilySize_cat_small  \n",
      "YP_008563134                      1                     0  \n",
      "XP_010327628                      0                     1  \n",
      "XP_010327620                      1                     0  \n",
      "XP_010327578                      1                     0  \n",
      "XP_010327494                      1                     0  \n",
      "\n",
      "[5 rows x 566 columns]\n",
      "\n",
      "Output file saved as: data_mod.txt\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# B. Drop/Impute NAs and one-hot-encode categorical features\n",
    "\n",
    "%run ../ML_preprocess.py -df data.txt -na_method median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the testing set data\n",
    "\n",
    "We want to set aside a subset of our data to use to test how well our model performed. Note that this is done before feature engineering, parameter selection, or model training. This will ensure our performance metric is entirely independent from our modeling!\n",
    "\n",
    "\n",
    "### The test_set.py script\n",
    "\n",
    "test_set.py will:\n",
    "- randomly select a subset of the instances (i.e. rows) to use for testing\n",
    "- For classification problems (i.e. this example), it will select the desired number (-n) or percent (-p) of instances from each class (specify -use if some classes shouldn't be in the test set, like unknown samples). \n",
    "- For regression problems (i.e. predicting plant height), it will randomly select the desired number (-n) or percent (-p) from the whole dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holding out 10.0 percent\n",
      "Pulling test set from classes: ['gen', 'special']\n",
      "285 instances in test set\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "# C. Define test set\n",
    "\n",
    "%run ../test_set.py -df data_mod.txt -use gen,special -type c -p 0.1 -save test_genes.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature engineering\n",
    "\n",
    "While one major advantage of ML approaches is that they are robust when the number of features is very large, there are cases where removing unuseful features or selecting only the best features may help you better answer your question. One common issue we see with using feature selection for machine learning is using the whole dataset to select the best features, which results in overfitting! Be sure to define a test set before feature selection is performed. \n",
    "\n",
    "\n",
    "Feature_Selection.py allows you to perform feature selection using a number of different feature selection algorithms including:\n",
    "\n",
    "- Enrichement (Fisher's Exact Test: fet) (for binary classification and binary features only)\n",
    "- Random Forest (rf)\n",
    "- L1/LASSO\n",
    "- Relief (Need to install ReBATE to run)\n",
    "- Bayesian LASSO (bl)(for regression only)\n",
    "- Elastic Net (EN)(for regression only)\n",
    "\n",
    "\n",
    "Here we will use one of the most common feature selection algorithms: LASSO. LASSO requires the user to select the level of sparcity (-p) they want to induce during feature selection, where a larger value will result in more features being selected and a smaller value resulting in fewer features being selected. You can play around with this value to see what it does for your data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping instances that are not in ['special', 'gen'], changed dimensions from (2872, 566) to (2850, 566) (instance, features).\n",
      "              Class  Crubella_183_v1.0.csv  FamilySize  Transferase  \\\n",
      "YP_008563134      0                    0.0    0.010582          0.0   \n",
      "XP_010327628      0                    0.0    0.000000          0.0   \n",
      "XP_010327620      1                    0.0    0.052910          0.0   \n",
      "XP_010327578      0                    0.0    0.015873          0.0   \n",
      "XP_010327494      0                    1.0    0.021164          0.0   \n",
      "\n",
      "              Exo_endo_phos  IPT  Paxillaris_medKaKs  GalKase_gal_bdg  \\\n",
      "YP_008563134            0.0  0.0            0.179007                0   \n",
      "XP_010327628            0.0  0.0            0.179007                0   \n",
      "XP_010327620            0.0  0.0            0.237404                0   \n",
      "XP_010327578            0.0  0.0            0.237404                0   \n",
      "XP_010327494            0.0  0.0            0.092387                0   \n",
      "\n",
      "              Pkinase_Tyr  Smelongena_medKaKs          ...           \\\n",
      "YP_008563134          0.0            0.030057          ...            \n",
      "XP_010327628          1.0            0.030057          ...            \n",
      "XP_010327620          1.0            0.064836          ...            \n",
      "XP_010327578          0.0            0.064836          ...            \n",
      "XP_010327494          0.0            0.085421          ...            \n",
      "\n",
      "              Glyco_hydro_28  Lipase_3  Wax2_C  Pabies_medKaKs  Glutaredoxin  \\\n",
      "YP_008563134               0         0       0        0.161359             0   \n",
      "XP_010327628               0         0       0        0.198686             0   \n",
      "XP_010327620               0         0       0        0.161359             0   \n",
      "XP_010327578               0         0       0        0.161359             0   \n",
      "XP_010327494               0         0       0        0.055363             0   \n",
      "\n",
      "              Osativa_medKaKs  Abhydrolase_6  FamilySize_cat_large  \\\n",
      "YP_008563134         0.247425              0                     0   \n",
      "XP_010327628         0.247425              0                     0   \n",
      "XP_010327620         0.206460              0                     0   \n",
      "XP_010327578         0.212226              0                     0   \n",
      "XP_010327494         0.179891              0                     0   \n",
      "\n",
      "              FamilySize_cat_medium  FamilySize_cat_small  \n",
      "YP_008563134                      1                     0  \n",
      "XP_010327628                      0                     1  \n",
      "XP_010327620                      1                     0  \n",
      "XP_010327578                      1                     0  \n",
      "XP_010327494                      1                     0  \n",
      "\n",
      "[5 rows x 566 columns]\n",
      "=====* Running L1/LASSO based feature selection *=====\n",
      "Features selected using LASSO: ['Transferase' 'p450' 'UDPGT' 'tandemDupGenes'\n",
      " 'Nicotiana_tabacum.TN90_AYMY.SS.csv' 'Ppatens_318_v3.3.csv'\n",
      " 'Atrichopoda_291_v1.0.csv' 'Coffea_canephora.csv' 'Nicotiana_tomen.csv'\n",
      " 'BrapaFPsc_277_v1.3.csv' 'FamilySize_cat_small']\n",
      "\n",
      "Number of features selected using LASSO (sparcity parameter = 0.01): 11\n",
      "Run time (sec):0.45\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "%run ../Feature_Selection.py -df data_mod.txt -cl_train special,gen -type c -alg lasso -p 0.01 -save top_feat_lasso.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Algorithm and parameter selection\n",
    "\n",
    "Next we want to determine which ML algorithm (i.e. Support Vector Machine (SVM), Random Forest (RF)) we should use and what parameters needed by those algorithms work best. Importantly, at this stage we **only assess our model performance on the validation data** in order to assure we aren't just selecting the algorithm that works best on our held out testing data. The pipeline will automatically withhold the testing data from the parameter selection (i.e. grid search) step. \n",
    "\n",
    "The machine learning algorithms in the ML_Pipeline are implement from [SciKit-Learn](https://scikit-learn.org/stable/), which has excellent resources to learn more about the ins and outs of these algorithms.\n",
    "\n",
    "Algorithms available in the pipeline are: Support Vector Machine (linear: SVM, polynomial: SVMpoly, radial basis function: SVMrbf), Random Forest (RF), Gradient Tree Boosting (GB), and Logistic Regression (LogReg).\n",
    "\n",
    "Note, there are many functions available within the pipeline that are not described in this workshop. Run python \"ML_classification.py -h\" to see more details!\n",
    "\n",
    "\n",
    "\n",
    "**Algorithm/Parameter Selection**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing test instances to apply model on later...\n",
      "Snapshot of data being used:\n",
      "                Class  Crubella_183_v1.0.csv  FamilySize  Transferase  \\\n",
      "YP_008563134      gen                    0.0    0.016667          0.0   \n",
      "XP_010327628      gen                    0.0    0.000000          0.0   \n",
      "XP_010327578      gen                    0.0    0.025000          0.0   \n",
      "XP_010327494      gen                    1.0    0.033333          0.0   \n",
      "YP_008563119  special                    0.0    0.000000          0.0   \n",
      "\n",
      "              Exo_endo_phos  \n",
      "YP_008563134            0.0  \n",
      "XP_010327628            0.0  \n",
      "XP_010327578            0.0  \n",
      "XP_010327494            0.0  \n",
      "YP_008563119            0.0  \n",
      "\n",
      "\n",
      "CLASSES: ['gen' 'special']\n",
      "POS: special <class 'str'>\n",
      "NEG: gen <class 'str'>\n",
      "\n",
      "Balanced dataset will include 478 instances of each class\n",
      "\n",
      "\n",
      "===>  Grid search started  <===\n",
      "Round 1 of 10\n",
      "Round 2 of 10\n",
      "Round 3 of 10\n",
      "Round 4 of 10\n",
      "Round 5 of 10\n",
      "Round 6 of 10\n",
      "Round 7 of 10\n",
      "Round 8 of 10\n",
      "Round 9 of 10\n",
      "Round 10 of 10\n",
      "Parameter sweep time: 19.352998 seconds\n",
      "Parameters selected: Kernel=Linear, C=0.5\n",
      "Grid search complete. Time: 19.356311 seconds\n",
      "\n",
      "\n",
      "===>  ML Pipeline started  <===\n",
      "  Round 1 of 10\n",
      "  Round 2 of 10\n",
      "  Round 3 of 10\n",
      "  Round 4 of 10\n",
      "  Round 5 of 10\n",
      "  Round 6 of 10\n",
      "  Round 7 of 10\n",
      "  Round 8 of 10\n",
      "  Round 9 of 10\n",
      "  Round 10 of 10\n",
      "ML Pipeline time: 12.837702 seconds\n",
      "\n",
      "\n",
      "===>  ML Results  <===\n",
      "\n",
      "Validation Set Scores\n",
      "Accuracy: 0.847908 (+/- stdev 0.007488)\n",
      "F1: 0.854258 (+/- stdev 0.006974)\n",
      "AUC-ROC: 0.913456 (+/- stdev 0.004399)\n",
      "AUC-PRC: 0.911292 (+/- stdev 0.006893)\n",
      "\n",
      "\n",
      "Test Set Scores:\n",
      "Precision: 0.816248\n",
      "Accuracy: 0.842050\n",
      "F1: 0.848241\n",
      "AUC-ROC: 0.897764 (+/- stdev 0.006935)\n",
      "AUC-PRC: 0.779631 (+/- stdev 0.013428)\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "%run ../ML_classification.py -df data_mod.txt -test test_genes.txt -cl_train special,gen -alg SVM -cv 5 -gs true -gs_reps 10 -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results Breakdown**\n",
    "\n",
    "The output here includes the results on both the validation data (selected randomly interally during the modeling process) and on the held out testing data. Since now we are just selecting the best algorithm/parameters, only look at the validation scores. \n",
    "\n",
    "\n",
    "** Comparing algorithms **\n",
    "\n",
    "Running the same script (only changing **-alg XXX**), performance on the validation data using other algorithms:\n",
    "\n",
    "| Alg  \t| F1  \t| AUC-ROC  \t|\n",
    "|---\t|---\t|---\t|\n",
    "| RF  \t|   \t|   \t|\n",
    "| LogReg  \t|   \t|   \t|\n",
    "| SVMpoly  \t|   \t|   \t|\n",
    "| SVM  \t| 0.848  \t| 0.898  \t|\n",
    "\n",
    "\n",
    "\n",
    "** performed best on the validation data so we will select that algorithm! **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model application\n",
    "\n",
    "Now that we have our best performing algorithm, we will run the pipeline one more time, but with more replicates and we will use it to predict our unknown genes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing test instances to apply model on later...\n",
      "Snapshot of data being used:\n",
      "                Class  Crubella_183_v1.0.csv  FamilySize  Transferase  \\\n",
      "YP_008563134      gen                    0.0    0.016667          0.0   \n",
      "XP_010327628      gen                    0.0    0.000000          0.0   \n",
      "XP_010327578      gen                    0.0    0.025000          0.0   \n",
      "XP_010327494      gen                    1.0    0.033333          0.0   \n",
      "YP_008563119  special                    0.0    0.000000          0.0   \n",
      "\n",
      "              Exo_endo_phos  \n",
      "YP_008563134            0.0  \n",
      "XP_010327628            0.0  \n",
      "XP_010327578            0.0  \n",
      "XP_010327494            0.0  \n",
      "YP_008563119            0.0  \n",
      "\n",
      "\n",
      "CLASSES: ['gen' 'special']\n",
      "POS: special <class 'str'>\n",
      "NEG: gen <class 'str'>\n",
      "\n",
      "Balanced dataset will include 478 instances of each class\n",
      "\n",
      "\n",
      "===>  Grid search started  <===\n",
      "Round 1 of 10\n",
      "Round 2 of 10\n",
      "Round 3 of 10\n",
      "Round 4 of 10\n",
      "Round 5 of 10\n",
      "Round 6 of 10\n",
      "Round 7 of 10\n",
      "Round 8 of 10\n",
      "Round 9 of 10\n",
      "Round 10 of 10\n",
      "Parameter sweep time: 37.327930 seconds\n",
      "Parameters selected: Kernel=Linear, C=0.5\n",
      "Grid search complete. Time: 37.335318 seconds\n",
      "\n",
      "\n",
      "===>  ML Pipeline started  <===\n",
      "  Round 1 of 50\n",
      "  Round 2 of 50\n",
      "  Round 3 of 50\n",
      "  Round 4 of 50\n",
      "  Round 5 of 50\n",
      "  Round 6 of 50\n",
      "  Round 7 of 50\n",
      "  Round 8 of 50\n",
      "  Round 9 of 50\n",
      "  Round 10 of 50\n",
      "  Round 11 of 50\n",
      "  Round 12 of 50\n",
      "  Round 13 of 50\n",
      "  Round 14 of 50\n",
      "  Round 15 of 50\n",
      "  Round 16 of 50\n",
      "  Round 17 of 50\n",
      "  Round 18 of 50\n",
      "  Round 19 of 50\n",
      "  Round 20 of 50\n",
      "  Round 21 of 50\n",
      "  Round 22 of 50\n",
      "  Round 23 of 50\n",
      "  Round 24 of 50\n",
      "  Round 25 of 50\n",
      "  Round 26 of 50\n",
      "  Round 27 of 50\n",
      "  Round 28 of 50\n",
      "  Round 29 of 50\n",
      "  Round 30 of 50\n",
      "  Round 31 of 50\n",
      "  Round 32 of 50\n",
      "  Round 33 of 50\n",
      "  Round 34 of 50\n",
      "  Round 35 of 50\n",
      "  Round 36 of 50\n",
      "  Round 37 of 50\n",
      "  Round 38 of 50\n",
      "  Round 39 of 50\n",
      "  Round 40 of 50\n",
      "  Round 41 of 50\n",
      "  Round 42 of 50\n",
      "  Round 43 of 50\n",
      "  Round 44 of 50\n",
      "  Round 45 of 50\n",
      "  Round 46 of 50\n",
      "  Round 47 of 50\n",
      "  Round 48 of 50\n",
      "  Round 49 of 50\n",
      "  Round 50 of 50\n",
      "ML Pipeline time: 103.517385 seconds\n",
      "\n",
      "\n",
      "===>  ML Results  <===\n",
      "\n",
      "Validation Set Scores\n",
      "Accuracy: 0.846151 (+/- stdev 0.008754)\n",
      "F1: 0.851530 (+/- stdev 0.008047)\n",
      "AUC-ROC: 0.912006 (+/- stdev 0.005970)\n",
      "AUC-PRC: 0.908622 (+/- stdev 0.010243)\n",
      "\n",
      "\n",
      "Test Set Scores:\n",
      "Precision: 0.811284\n",
      "Accuracy: 0.834728\n",
      "F1: 0.840726\n",
      "AUC-ROC: 0.897890 (+/- stdev 0.007517)\n",
      "AUC-PRC: 0.776094 (+/- stdev 0.015314)\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "%run ../ML_classification.py -df data_mod.txt -test test_genes.txt -cl_train special,gen -apply unknown -alg SVM -cv 5 -gs true -gs_reps 10 -n 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results Breakdown**\n",
    "\n",
    "First, note that before the grid search started running, the model told you what your positive and negative class strings were and how many instances would be in each class for each replicate. This is an important feature in the ML-Pipeline. Training a ML classifier with unbalanced data, or data with different numbers of each instance, can cause your model to be biased toward predicting the more numerous class. For example, if you train a model using 100 positive examples and 100,000 negative examples, your model would do well to just call instance negative! Therefore, in this pipeline, the larger classes are randomly downsampled to generate balanced datasets. To ensure we still utilize as much data as possible, this downsampling is done independelty for each replicate (-n). \n",
    "\n",
    "A number of performance metrics are included for binary classification models, including [AUC-ROC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html), [F-measure](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html), [AUC-PRC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html), [accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html), and [precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score).\n",
    "\n",
    "**Output files**\n",
    "\n",
    "The classficiation pipeline generates similar output as the regression pipeline, although there are a few notable differences. \n",
    "\n",
    "- **\"_scores\":** Here, the predicted probability (pp) scores are reported for each replicate. The pp score represents how confident the model was in its classification, where a pp=1 means it is certain the instance is positive and pp=0 means it is certain the instance is negative. For each replicate, an instance is classified as pos if pp > threshold, which is defined as value between 0.001-0.999 that maximises the F-measure. While the performance metrics generated by the pipeline are calcuated for each replicate independently, we want to be able to make a final statement about which instances were called as positive and which were called as negative. You'll find those results in this file. To make this final call we calculated the mean threshold and the mean pp for each instance and called the instance pos if the mean pp > mean threshold. \n",
    "- **\"_results\":** Here you will see an overview of the results similar to what is printed in the command line. However, for classification problems you will see two additional sections: the Mean Balanced Confusion Matrix (CM and the Final Full CM. The mean balanced CM was generated by taking the average number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) across all replicate (which have been downsampled randomly to be balanced). The Final Full CM represents the final TP, TN, FP, and FN results from the final pos/neg classifications (descirbed above in _scores) for all instances in your input dataset. \n",
    "- **\"_BalancedID\":** Each row lists the instances that were included in each replicate (-n) after downsampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../ML_classification.py -df data_multiclass.txt -test test_mc -y_name yield \\\n",
    "-cl_train top,middle,bottom -alg SVM -gs true -gs_reps 2 -n 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results Breakdown**\n",
    "\n",
    "The output for multi-class classification models is similar to binary classification models with a few key differences. \n",
    "\n",
    "*An important note: For binary classification using balanced datasets, you would expect a ML model that was just randomly guessing the class to be correct ~50% of the time, because of this the random expectation for performance metrics like AUC-ROC and the F-measure are 0.50. This is not the case for multi-class predictions. Using our model above as an example, a ML model that was randomly guessing top, middle, or bottom, would only be correct ~33% of the time. That means models performing with >33% accuracy are performing better than random expectation.*\n",
    "\n",
    "There are two types of performance metrics for multi-class models, commonly referred to as macro and micro metrics. Micro performance metrics are generated for each class in your ML problem. For example, from our model we will get three micro F-measures (F1-top, F1-middle, F1-bottom). These micro scores are available in the *_results* output file. Macro performance metrics are generated by taking the average of all the micro performance metrics. These scores are what are printed in the command line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.C Applying ML model to unknowns\n",
    "\n",
    "Because the ML-Pipeline generates many many models to make one prediction (e.g. 100 replicates with 10-fold cross valiation is like having 1,000 seperate models) the pipeline does not \"save\" the models. However, you can still use the pipeline to apply your trained model to unknown instances. Unknown instances must have the same features as your training data. In this tutorial for example, we would need to know transcript levels for all of the same genes, but we would not need to know the yield before hand.\n",
    "\n",
    "Start by simply adding your unknown instances to the input dataframe. In the column that has the value or class (Y/Class) for the known instances, put a different identifier (e.g. \"unknown\").\n",
    "\n",
    "For this example, we've generated a version of the regression dataset (data_regression_unk.txt) where the first 20 lines were designated as unknowns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../ML_classification -df data_binary.txt -test test_bin -y_name yield \\\n",
    "-alg SVM -gs true -gs_reps 2 -n 2 -feat feat_lasso_0.02 -save data_binary_SVM_top15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we ran SVM to classify lines using all 200 transcript features we got an F-measure (F1) = 0.747, so you can see that we did nearly as well just using the top 15 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_reg = pd.read_csv('data_regression_unk.txt', sep='\\t', index_col = 0)\n",
    "print(d_reg.ix[:8,:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run the ML-Pipeline again, but this time specifying that we want to apply the models to the unknowns (-apply unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../ML_regression.py -df data_regression_unk.txt -test test_reg -y_name yield \\\n",
    "-alg RF -gs true -gs_reps 2 -n 2 -apply unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the predicted yield for those five lines, check out the \"_scores\" file that was just generated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.D Visualizing Your Results\n",
    "\n",
    "There are a number of vizualization tools available in the ML-Pipeline. We will describe just some of them here!\n",
    "\n",
    "**Generate Plots while training and testing your models**\n",
    "\n",
    "1. One optional parameter for both the ML_classification.py and ML_regression.py pipelines is -plots T/F. The default is False, but if you want to generate some plots directly during your ML run, set -plots T. For classification you will get an [AUC-ROC plot](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5) and an [AUC-PRC plot](https://classeval.wordpress.com/introduction/introduction-to-the-precision-recall-plot/). \n",
    "\n",
    "2. In the ML_classification.py pipeline, another plotting option is to auto-generate a [confusion matrix heatmap](https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62) (-cm), just set -cm T\n",
    "\n",
    "**Generate plots post-model building**\n",
    "1. **ML_plots.py** allows you to generate AUC-ROC and AUC-PRC plots from multiple runs, for example if you wanted to compare performance of different algorithms or using different sets of features. For example, we can compare the results from our classification model with all 200 features with the one generated using the top 15 features:\n",
    "\n",
    "```\n",
    "ML_plots.py [SAVE_NAME] [POS] [NEG] [M1_name] [PATH_M1_scores] [M2_name] [PATH_M2_scores]... [Mn_name] [PATH_Mn_scores]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../ML_plots.py binary_plots 1 0 all data_binary.txt_SVM_scores.txt top15 data_binary_SVM_top15_scores.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **compare_classifiers.py** takes in the results from different classification models and generates a list of which instances were correctly and incorrectly predicted by each model and generates a venn diagram showing the overlap.\n",
    "\n",
    "```\n",
    "ML_plots.py -scores PATH_M1_scores,PATH_M2_scores,...,PATH_Mn_scores -ids M1_name,M2_name,...,Mn_name -save [SAVE_NAME]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../compare_classifiers.py  -scores data_binary.txt_SVM_scores.txt,data_binary_SVM_top15_scores.txt -ids all,top15 -save binary_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results indicate that our models using all and the top 15 features both predicted 88 instances correctly as positive, but only 82 of those instances were the same for both models, meaning they each uniquely correctly classified 6 instances as positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Topics:\n",
    "\n",
    "- One parameter that can be adjusted in the ML_classification.py or ML_regression.py script is how many cross validation folds you want to include (-cv). The default and a commonly used fold number is -cv 10, however, if you have a small dataset, using fewer folds (-cv 5) may perform better. In the extreme case, if you have very few instances to train on, you can set -cv equal to the number of instances in your dataset, allowing you to perform leave one out LOO cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **plot_predprob.py**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
